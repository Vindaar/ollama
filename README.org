* Ollama interface

This library is just a simple Ollama interface for the server running
in on your local machine.

Fittingly, it was actually just generated by GPT4o by me pasting the
entire Ollama docs and asking it to write a Nim wrapper. Only had to
fix one thing. Pretty neat how far we've come in LLMs writing Nim,
lol. I just made a few tiny changes (in particular turning the client
into a ref object and defining a destructor to close the ~HttpClient~).


*Note*: I haven't looked into handling streamed responses. So better
use ~stream = false~ for now or create a PR to fix that. :)

#+begin_src nim
let client = newOllamaClient()
let response = generateCompletion(client, "llama3", "Why is the sky blue?")
echo response.pretty
#+end_src nim
